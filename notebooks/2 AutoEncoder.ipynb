{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from dvclive.lightning import DVCLiveLogger\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from src.data import OpenNeuroDataModule\n",
    "from src.models import MultiHeadClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 24, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 24, 10]),\n",
       " torch.Size([64, 12]),\n",
       " torch.Size([64, 2]),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule = OpenNeuroDataModule(batch_size=2)\n",
    "import torch\n",
    "# this is a test\n",
    "model = MultiHeadClassifier(n_subjects=datamodule.n_subjects,\n",
    "                            n_spatial_features=12,\n",
    "                            n_temporal_features=12,\n",
    "                            example_input_array=datamodule.get_example_batch())\n",
    "\n",
    "# x = datamodule.get_example_batch()[0]\n",
    "x = torch.rand(64, 24, 10)\n",
    "x_recon, h, y_cls, y_subj = model(x)\n",
    "\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=1, accelerator='auto', log_every_n_steps=1,\n",
    "#     logger=[DVCLiveLogger(save_dvc_exp=True, report='md'),\n",
    "#             TensorBoardLogger('.')]\n",
    "#     )\n",
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 10, 3]) torch.Size([1, 7, 3]) torch.Size([1, 7, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 7, 10), got [1, 7, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/morteza/workspace/MultiModalRest/notebooks/2 AutoEncoder.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morteza/workspace/MultiModalRest/notebooks/2%20AutoEncoder.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mprint\u001b[39m(y_t_enc\u001b[39m.\u001b[39mshape, h_t_enc\u001b[39m.\u001b[39mshape, c_t_enc\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morteza/workspace/MultiModalRest/notebooks/2%20AutoEncoder.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     x_t_dec \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(batch_size, n_timepoints, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/morteza/workspace/MultiModalRest/notebooks/2%20AutoEncoder.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     y_t_dec, (h_t_dec, c_t_dec) \u001b[39m=\u001b[39m time_dec(x_t_dec, (h_t_enc, c_t_enc))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morteza/workspace/MultiModalRest/notebooks/2%20AutoEncoder.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     x_ch \u001b[39m=\u001b[39m space_dec(y_t_dec)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morteza/workspace/MultiModalRest/notebooks/2%20AutoEncoder.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(h, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/MultiModalRest/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/MultiModalRest/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/MultiModalRest/lib/python3.11/site-packages/torch/nn/modules/rnn.py:875\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    872\u001b[0m             hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m    873\u001b[0m         \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    874\u001b[0m         \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m--> 875\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/MultiModalRest/lib/python3.11/site-packages/torch/nn/modules/rnn.py:791\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    786\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    787\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    788\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    789\u001b[0m                        ):\n\u001b[1;32m    790\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 791\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_hidden_size(hidden[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_expected_hidden_size(\u001b[39minput\u001b[39;49m, batch_sizes),\n\u001b[1;32m    792\u001b[0m                            \u001b[39m'\u001b[39;49m\u001b[39mExpected hidden[0] size \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m, got \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    794\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/MultiModalRest/lib/python3.11/site-packages/torch/nn/modules/rnn.py:256\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_hidden_size\u001b[39m(\u001b[39mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[1;32m    254\u001b[0m                       msg: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mExpected hidden size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m expected_hidden_size:\n\u001b[0;32m--> 256\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(expected_hidden_size, \u001b[39mlist\u001b[39m(hx\u001b[39m.\u001b[39msize())))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 7, 10), got [1, 7, 3]"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "n_timepoints = 10\n",
    "n_channels = 8\n",
    "spatial_features = 4\n",
    "temporal_features = 3\n",
    "\n",
    "import torch\n",
    "from torch.nn import LSTM, Conv1d, ConvTranspose1d\n",
    "\n",
    "x_flat = torch.rand(batch_size, n_timepoints, n_channels).permute(0, 2, 1)\n",
    "\n",
    "space_enc = Conv1d(n_channels, spatial_features, kernel_size=1)\n",
    "time_enc = LSTM(1, temporal_features, batch_first=True)  \n",
    "time_dec = LSTM(1, n_timepoints, batch_first=True)  \n",
    "space_dec  = ConvTranspose1d(spatial_features, n_channels, kernel_size=1)\n",
    "\n",
    "y_space_enc = space_enc(x_flat).permute(0, 2, 1)\n",
    "\n",
    "h = []\n",
    "\n",
    "for ch in range(spatial_features):\n",
    "    x_ch = y_space_enc[:, :, ch].unsqueeze(-1)\n",
    "    y_t_enc, (h_t_enc, c_t_enc) = time_enc(x_ch)\n",
    "    h.append(h_t_enc[0,...])\n",
    "    print(y_t_enc.shape, h_t_enc.shape, c_t_enc.shape)\n",
    "    x_t_dec = torch.zeros(batch_size, n_timepoints, 1)\n",
    "    y_t_dec, (h_t_dec, c_t_dec) = time_dec(x_t_dec, (h_t_enc, c_t_enc))\n",
    "    x_ch = space_dec(y_t_dec)\n",
    "\n",
    "h = torch.stack(h, dim=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MultiModalRest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
